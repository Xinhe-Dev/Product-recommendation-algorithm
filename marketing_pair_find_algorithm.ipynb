{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSXfAf8KdeIV",
        "outputId": "3dda35f4-bfc1-43e0-baec-862cb0e2a0f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 39.7 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 124950 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u432-ga~us1-0ubuntu2~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u432-ga~us1-0ubuntu2~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive2\n",
        "\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MOII1NXjdy4J"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# create the Spark Session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# create the Spark Context for working with RDDs\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aeuf4X4lAaQ_"
      },
      "outputs": [],
      "source": [
        "def my_combinations(sorted_list, k):\n",
        "  \"\"\"\n",
        "  Generate all combinations of size k from sorted_list,\n",
        "  *without* using itertools.\n",
        "  \"\"\"\n",
        "  n = len(sorted_list)\n",
        "\n",
        "  #doing a backtracking approach\n",
        "  def backtrack(start, chosen):\n",
        "    if len(chosen) == k:\n",
        "      yield tuple(chosen)\n",
        "      return\n",
        "    #we can only pick elements from index start..(n - (k-len(chosen)))\n",
        "    limit=n-(k-len(chosen))+1\n",
        "    for i in range(start,limit):\n",
        "      chosen.append(sorted_list[i])\n",
        "      yield from backtrack(i+1,chosen)\n",
        "      chosen.pop()\n",
        "\n",
        "  yield from backtrack(0,[])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHZ1iIxa4wMf",
        "outputId": "a1cd988e-4c84-43ed-8a19-48189ec3601d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of transactions:  10000\n"
          ]
        }
      ],
      "source": [
        "# Specify the filenames\n",
        "transactions_filename = 'walmart1.csv'\n",
        "item_names_filename = 'ID2Name.csv'\n",
        "\n",
        "# Read transactions into an RDD\n",
        "#here is worth to mention:\n",
        "#if we just simply reading the dataset to transaction\n",
        "#it will record all the item happened inside in data set e.g ({1,4,4} it will count 4 two time)\n",
        "#which we only want count number 4 once\n",
        "#therefore we convert it to line then put into transcation, it will only count 4 one time\n",
        "transactions=sc.textFile(transactions_filename)\\\n",
        "  .map(lambda line: [int(x) for x in line.strip().split(',')])\\\n",
        "  .map(lambda items: list(set(items)))\\\n",
        "  .filter(lambda items: len(items) > 0)\\\n",
        "  .cache()\n",
        "\n",
        "# Read the item ID2Name file into a Python dictionary\n",
        "lines=sc.textFile(item_names_filename)\n",
        "\n",
        "#build a dictionary('1324', 'item name')\n",
        "item_id_to_name=(\n",
        "    lines\n",
        "    .map(lambda x:x.split(',',1))\n",
        "    .map(lambda pair:(int(pair[0]),pair[1]))\n",
        "    .collectAsMap()\n",
        ")\n",
        "\n",
        "# Print the number of transactions  (You can keep it in a variable so that you can use it later)\n",
        "num_transactions=transactions.count()#keep it variable :)\n",
        "print(\"Number of transactions: \", num_transactions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcpY2IUelDC8"
      },
      "source": [
        "\n",
        "Function to calculate frequent 1-itemsets using RDD operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIKkWY3blIUm"
      },
      "outputs": [],
      "source": [
        "def find_frequent_1_itemsets(transactions, min_support_count:int):\n",
        "  '''\n",
        "  :@transactions: RDD of transactions\n",
        "  :@min_support_count: the minimum number of transactions an itemset must appear to be frequent\n",
        "  '''\n",
        "  #1.count each individual item's frequency\n",
        "  #2.Filter out items not meeting min_support_count\n",
        "  #3.return as a list in this format [(item,), count)...]\n",
        "    #here just print a list for all 1-k items which all satisfy the min_sup\n",
        "\n",
        "  item_counts=(transactions #the transaction here is RDD!\n",
        "               .flatMap(lambda t:[(item,1) for item in t]) #map each transaction 't' to am (item,1)\n",
        "               .reduceByKey(lambda x,y:x+y)  #accumlating same item\n",
        "               .filter(lambda x:x[1]>=min_support_count) #filter any thing less than min_sup\n",
        "               .collect()) #fet a python list\n",
        "\n",
        "  #sort item's IDS to keep consistrency, and put them in tuple form\n",
        "  frequent_1_itemsets=[((item_count[0],),item_count[1]) for item_count in item_counts]\n",
        "\n",
        "  return frequent_1_itemsets  #return the frequent-itemsets of length 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lI5NFB6lLJW"
      },
      "source": [
        "Function to generate candidates of size k from frequent itemsets of size k-1.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHBHxHBJlZAY"
      },
      "outputs": [],
      "source": [
        "def generate_candidates(frequent_itemsets:list, k:int):\n",
        "  '''\n",
        "  :@frequent_itemsets: frequent itemsets of length k-1, which is the output of find_frequent_1_itemsets/find_frequent_k_itemsets\n",
        "  :@k: the size of candidate itemsets to generate\n",
        "  '''\n",
        "  #approach\n",
        "  #1.extract just the itemsets(ignore the count here)\n",
        "  #2.join pairs that share the first k-2 items in sorted order\n",
        "  #3.purne by ensuring all (k-1)-subsets of a condidate are frequent\n",
        "\n",
        "  #extract just the itemsets\n",
        "  freq_itemsets_k_1=[sorted(itemset_count[0]) for itemset_count in frequent_itemsets]\n",
        "  freq_itemsets_k_1=list(set(tuple(x) for x in freq_itemsets_k_1)) #unique set\n",
        "\n",
        "  #sort the list so we can have a consistent ordering\n",
        "  freq_itemsets_k_1.sort()\n",
        "\n",
        "  candidates_k = set()\n",
        "  length = k - 1\n",
        "\n",
        "\n",
        "  #join step\n",
        "  for i in range(len(freq_itemsets_k_1)):\n",
        "    for j in range(i+1,len(freq_itemsets_k_1)):\n",
        "      #try to merge only if the first k-1 item is match\n",
        "      L1=freq_itemsets_k_1[i]\n",
        "      L2=freq_itemsets_k_1[j]\n",
        "\n",
        "      # only merge if they share first (k-2) items\n",
        "      if L1[:length-1]==L2[:length-1]:\n",
        "        #merge them\n",
        "        merged = sorted(list(set(L1).union(set(L2))))\n",
        "        if len(merged)==k:\n",
        "          #prune step! all (k-1) subsets of merged must be frequent\n",
        "          all_subsets_frequent=True\n",
        "          for idx in range(k):\n",
        "            #build the (k-1)subset by removeing item at position idx\n",
        "            subset=merged[:idx]+merged[idx+1:]\n",
        "            #check if that subset is in freq_itemsets_k_1\n",
        "            if tuple(subset) not in freq_itemsets_k_1:\n",
        "              all_subsets_frequent=False\n",
        "              break\n",
        "          if all_subsets_frequent:\n",
        "            candidates_k.add(tuple(merged))\n",
        "\n",
        "  return candidates_k  # return the set of candidates of length k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRlKNr4SlbLu"
      },
      "source": [
        "Function to compute the support count of each candidate and filter out infrequent candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JL6GHVeinQ--"
      },
      "outputs": [],
      "source": [
        "def find_frequent_k_itemsets(transactions, candidates, min_support_count):\n",
        "  '''\n",
        "  :@transactions: RDD of transactions\n",
        "  :@candidates: the set of candidates to be considered for frequent itemsets\n",
        "  :@min_support_count: the minimum number of transactions an itemset must appear to be frequent\n",
        "  '''\n",
        "  #quick stop if it not candidates, easy way to save\n",
        "  if not candidates:\n",
        "    return []\n",
        "\n",
        "  #store the data in local variable\n",
        "  candidate_set=set(tuple(sorted(c))for c in candidates)\n",
        "\n",
        "  #find k by taking one candidate from candidate_set\n",
        "  sample_candidate=next(iter(candidate_set))\n",
        "  k=len(sample_candidate)\n",
        "\n",
        "  def mapper(trans):\n",
        "    #sort the transaction once here if needed\n",
        "    trans_sorted = sorted(trans)\n",
        "    out = []\n",
        "\n",
        "    for combo in my_combinations(trans_sorted, k):\n",
        "      if combo in candidate_set:\n",
        "        out.append((combo, 1))\n",
        "    return out\n",
        "\n",
        "  candidate_counts=(transactions\n",
        "          .flatMap(mapper)\n",
        "          .reduceByKey(lambda x,y:x+y)\n",
        "          .filter(lambda x:x[1]>=min_support_count)\n",
        "          .collect())\n",
        "\n",
        "\n",
        "  frequent_k_itemsets=[\n",
        "      (tuple(sorted(itemset_count[0])),itemset_count[1])\n",
        "      for itemset_count in candidate_counts\n",
        "  ]\n",
        "\n",
        "  return frequent_k_itemsets # return the frequent itemsets of length k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2BMZ46aqkCE"
      },
      "source": [
        "Apriori algorithm: use the above functions to find frequent itemsets of all lengths given a RDD of transactions and a minumum support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dryn10Oqous"
      },
      "outputs": [],
      "source": [
        "def apriori(transactions, min_support):\n",
        "  '''\n",
        "  :@transactions: RDD of transactions\n",
        "  :@min_support: the minimum support threshold for an itemset to be frequent\n",
        "  '''\n",
        "  #compute min_support_count\n",
        "  num_trans=transactions.count() #already define earlier, but that one doesn't work(attentation here)\n",
        "  min_support_count=int(min_support*num_trans)\n",
        "\n",
        "  #find frequent 1-items\n",
        "  freq_1_itemsets=find_frequent_1_itemsets(transactions, min_support_count)\n",
        "\n",
        "  frequent_itemsets_all=[] #check the return value\n",
        "  current_freq_itemsets=freq_1_itemsets\n",
        "\n",
        "  k = 1\n",
        "\n",
        "\n",
        "  #add transaction reduction: remove infrequent items\n",
        "  if current_freq_itemsets:\n",
        "    frequent_itemsets_all.append(current_freq_itemsets)\n",
        "    freq_1_items=set(x[0][0] for x in current_freq_itemsets)\n",
        "    transactions=transactions.map(lambda t: [i for i in t if i in freq_1_items])\\\n",
        "                               .filter(lambda t: len(t) > 0)\\\n",
        "                               .cache()\n",
        "  else:\n",
        "    return frequent_itemsets_all #no freq singletons\n",
        "\n",
        "  #iteratively generate freq. itemsets\n",
        "  while len(current_freq_itemsets)>0:\n",
        "    k+=1\n",
        "    candidates_k=generate_candidates(current_freq_itemsets,k)\n",
        "    if len(candidates_k)==0:\n",
        "      break\n",
        "\n",
        "    freq_k_itemsets=find_frequent_k_itemsets(transactions,candidates_k,min_support_count)\n",
        "    if len(freq_k_itemsets)==0:\n",
        "      break\n",
        "\n",
        "    frequent_itemsets_all.append(freq_k_itemsets)\n",
        "    current_freq_itemsets=freq_k_itemsets\n",
        "\n",
        "  return frequent_itemsets_all  # return the frequent itemsets of all lengths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Nh1RQRCFCIR"
      },
      "source": [
        "Print all frequent itemsets using item IDs and their support counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NrQ41DELFEtZ"
      },
      "outputs": [],
      "source": [
        "# For each level (length), print IDs of frequent itemsets\n",
        "def print_frequent_itemsets(frequent_itemsets_all):\n",
        "  '''\n",
        "  :@frequent_itemsets_all: the output of the apriori function (list of lists. Each inner list is a list of tuples)\n",
        "  '''\n",
        "  # Your code\n",
        "  for i, freq_list in enumerate(frequent_itemsets_all):\n",
        "    print(f\"Frequent itemsets of length {i+1}:\")\n",
        "    for (itemset, count) in freq_list:\n",
        "      print(f\" {tuple(str(x) for x in itemset)}:{count}\") #print out for manually check result\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nasez8iIk5_G"
      },
      "source": [
        "Function to generate strong association rules and remove misleading rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0LLWp-ftORk"
      },
      "outputs": [],
      "source": [
        "def generate_association_rules(frequent_itemsets_all, min_confidence):\n",
        "  '''\n",
        "  :@frequent_itemsets: the output of the apriori function\n",
        "  :@min_confidence: the minimum confidence for a rule to be considered interesting\n",
        "  '''\n",
        "  # generate the association rules from the frequent itemsets\n",
        "  # rules are of the form (X, Y, support, confidence) which means X => Y [support, confidence]\n",
        "  # build a dictionary for support_count of every frequent itemset, so we can quickly\n",
        "  # look up support count. We will store support_dict[frozenset(itemset)]=count\n",
        "  global num_transactions\n",
        "  support_dict={}\n",
        "  for freq_list in frequent_itemsets_all:\n",
        "    for (items,cnt) in freq_list:\n",
        "      support_dict[frozenset(items)]=cnt\n",
        "\n",
        "  # actually we must use the real transaction count, not sum the freq list (remember this failed attempt)\n",
        "  # i will fix that, i think i will neeed the support as fraction=cpunt/num_transaction,\n",
        "  # but we only know the actual #transcation from outside, so let's store absolute count\n",
        "  # abd do fraction later\n",
        "  rules=[]\n",
        "  # a helper function to get all non-empty subsets of a given set\n",
        "  def all_nonempty_subsets(items):\n",
        "    s=list(items)\n",
        "    n = len(s)\n",
        "    for r in range(1, n): # skip the full set\n",
        "      for combo in my_combinations(s, r):\n",
        "        yield combo\n",
        "\n",
        "  # prune the misleading rules by using the lift measure\n",
        "  # for each frequent itemset F of size >=2, we generate all non-empty subsets X\n",
        "  # then Y=F\\X. We cna compute confidence=support(F)/support(X)\n",
        "  # if confindence >=min_confidence, we can keep the rule\n",
        "  # then we also do lift=confidence/p(Y)=[support(F)/support(X)]/[support(Y)/N]\n",
        "  # we can keep the rule if lift > 1, which is considered as corraltion\n",
        "\n",
        "\n",
        "  #here I also compare the min_confidence, since is fast way to pass the meaningless rule\n",
        "  #go through each level of frequent itemsets\n",
        "  for freq_list in frequent_itemsets_all:\n",
        "    for (itemset_tuple, cntF) in freq_list:\n",
        "      F=frozenset(itemset_tuple)\n",
        "      supportF=cntF\n",
        "\n",
        "      for subset in all_nonempty_subsets(F):\n",
        "        Y=frozenset(subset)\n",
        "        X=F-Y\n",
        "\n",
        "        if not X:\n",
        "          continue\n",
        "\n",
        "        supportX=support_dict.get(X,0)\n",
        "        if supportX==0:\n",
        "          continue  #avoid division as 0\n",
        "\n",
        "\n",
        "        confidence=float(supportF)/float(supportX)\n",
        "\n",
        "        if confidence>=min_confidence:\n",
        "          supportY=support_dict.get(Y,0)\n",
        "          if supportY==0:\n",
        "            continue\n",
        "\n",
        "          lift=confidence/(float(supportY)/float(num_transactions))\n",
        "\n",
        "          if lift>1.0:\n",
        "            rule_support=float(supportF)/float(num_transactions)\n",
        "            #sort each side for consistency in printing\n",
        "            X_sorted = tuple(sorted(X))\n",
        "            Y_sorted = tuple(sorted(Y))\n",
        "            rules.append((X_sorted, Y_sorted, rule_support, confidence))\n",
        "\n",
        "\n",
        "  # return a list of rules\n",
        "  return rules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTrB4r3A5A2P"
      },
      "source": [
        "Print the rules using both item IDs and item names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQogZDEd5DRm"
      },
      "outputs": [],
      "source": [
        "def print_association_rules(association_rules):\n",
        "  '''\n",
        "  :@association_rules: the output of generate_association_rules\n",
        "  '''\n",
        "  # Print the rules using both item IDs, e.g., ('2064',) --> ('2828',) [support: 0.0314, confidence: 0.6767241379310345]\n",
        "  print('******** ASSOCIATION RULES with ITEM IDs ********')\n",
        "  for(X, Y, sup, conf) in association_rules:\n",
        "    print(f\"{tuple(str(x) for x in X)} --> {tuple(str(y) for y in Y)} [support: {sup:.4f}, confidence: {conf}]\")\n",
        "\n",
        "  # Print the rules using both item names, e.g., JUMBO BAG RED RETROSPOT,  --> DOTCOM POSTAGE, [support: 0.0314, confidence: 0.6767241379310345]\n",
        "  print('\\n******** ASSOCIATION RULES with ITEM NAMES ********')\n",
        "  for (X, Y, sup, conf) in association_rules:\n",
        "    X_names = [item_id_to_name.get(x, x) for x in X]\n",
        "    Y_names = [item_id_to_name.get(y, y) for y in Y]\n",
        "    print(f\"{', '.join(X_names)}, --> {', '.join(Y_names)}, [support: {sup:.4f}, confidence: {conf}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dz8pTJ1t5i0"
      },
      "source": [
        "Recommendation with association rules: given a transaction, recommend zero or items according to the association_rules based on the given recommendation algorithm. The function should print out the fired rules (using item IDs) and return the list of recommended items (using item IDs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfEWvGSyuuuS"
      },
      "outputs": [],
      "source": [
        "def recommend(transaction, association_rules):\n",
        "  '''\n",
        "  :@transaction: a list of item IDs\n",
        "  :@association_rules: the output of generate_assocication_rules\n",
        "  '''\n",
        "  trans_set=set(transaction)\n",
        "\n",
        "  print('*** Rules fired ***')\n",
        "  fired_rules=[]\n",
        "  recommendations=set()\n",
        "\n",
        "  #LHS & RHS search, which subset search\n",
        "  for (X, Y, sup, conf) in association_rules:\n",
        "    X_set = set(X)\n",
        "    if X_set.issubset(trans_set):\n",
        "      fired_rules.append((X, Y))\n",
        "      for y_item in Y:\n",
        "        if y_item not in trans_set:\n",
        "          recommendations.add(y_item)\n",
        "  for (X, Y) in fired_rules:\n",
        "    print(f\"{tuple(str(x) for x in X)} --> {tuple(str(y) for y in Y)}\")\n",
        "\n",
        "  return list(recommendations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ8ZBNDW3ToX"
      },
      "source": [
        "Print the names of the recommended items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-FKIXiy3XzP"
      },
      "outputs": [],
      "source": [
        "def print_recommended_items(recommended_items):\n",
        "  '''\n",
        "  :@recommended_items: the output of recommend\n",
        "  '''\n",
        "  print('\\nRecommended item IDs:')#print ID\n",
        "  for rid in recommended_items:\n",
        "    print(rid)\n",
        "\n",
        "  print('\\nRecommended item names:')#print ID's name\n",
        "  for rid in recommended_items:\n",
        "    print(item_id_to_name.get(rid, rid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjQvuYbGq8SM"
      },
      "source": [
        "Main program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iymb9LT92RTo"
      },
      "outputs": [],
      "source": [
        "# Set the support and confidence thresholds (the values can be changed)\n",
        "min_support = 0.025\n",
        "min_confidence = 0.6\n",
        "\n",
        "# Run Apriori\n",
        "frequent_itemsets_all = apriori(transactions, min_support)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqQGenUw_2xE",
        "outputId": "8ee9572a-3c35-4d1a-ee9e-4c694fd0fc8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frequent itemsets of length 1:\n",
            " ('2048',):353\n",
            " ('2050',):427\n",
            " ('2060',):254\n",
            " ('2064',):464\n",
            " ('4146',):266\n",
            " ('4152',):704\n",
            " ('4158',):429\n",
            " ('4200',):539\n",
            " ('4226',):567\n",
            " ('4240',):336\n",
            " ('4258',):611\n",
            " ('2238',):286\n",
            " ('4304',):287\n",
            " ('4404',):301\n",
            " ('4410',):275\n",
            " ('4468',):275\n",
            " ('2442',):311\n",
            " ('4514',):385\n",
            " ('4518',):349\n",
            " ('4520',):250\n",
            " ('4528',):268\n",
            " ('2568',):594\n",
            " ('4616',):270\n",
            " ('4632',):255\n",
            " ('2628',):451\n",
            " ('4700',):614\n",
            " ('2670',):489\n",
            " ('2706',):479\n",
            " ('2720',):426\n",
            " ('2794',):639\n",
            " ('2816',):521\n",
            " ('2818',):304\n",
            " ('2826',):654\n",
            " ('2828',):1069\n",
            " ('2832',):455\n",
            " ('2838',):470\n",
            " ('4946',):443\n",
            " ('3008',):289\n",
            " ('3010',):562\n",
            " ('3012',):348\n",
            " ('3014',):553\n",
            " ('3018',):539\n",
            " ('3020',):473\n",
            " ('3024',):505\n",
            " ('1098',):469\n",
            " ('1104',):488\n",
            " ('1108',):352\n",
            " ('1138',):384\n",
            " ('1142',):497\n",
            " ('1228',):320\n",
            " ('1296',):401\n",
            " ('3338',):389\n",
            " ('3358',):335\n",
            " ('1322',):254\n",
            " ('3404',):321\n",
            " ('3432',):1031\n",
            " ('1540',):316\n",
            " ('3672',):323\n",
            " ('3678',):503\n",
            " ('1696',):381\n",
            " ('1718',):256\n",
            " ('3784',):298\n",
            " ('1750',):281\n",
            " ('3828',):503\n",
            " ('3836',):285\n",
            " ('3842',):297\n",
            " ('1834',):455\n",
            " ('3918',):384\n",
            " ('1886',):289\n",
            " ('3970',):613\n",
            " ('2038',):250\n",
            " ('2044',):268\n",
            " ('4094',):262\n",
            " ('1220',):722\n",
            " ('3710',):518\n",
            " ('4920',):315\n",
            " ('3804',):374\n",
            " ('1844',):400\n",
            " ('3484',):420\n",
            " ('1140',):276\n",
            " ('4512',):475\n",
            " ('3356',):295\n",
            " ('2242',):259\n",
            " ('2236',):359\n",
            " ('5040',):258\n",
            " ('3400',):284\n",
            " ('2840',):356\n",
            " ('3808',):292\n",
            " ('2764',):340\n",
            " ('3236',):339\n",
            " ('2052',):282\n",
            " ('2054',):399\n",
            " ('4942',):318\n",
            " ('3402',):420\n",
            " ('3212',):269\n",
            " ('4151',):399\n",
            " ('2129',):319\n",
            " ('2173',):259\n",
            " ('4267',):288\n",
            " ('4287',):455\n",
            " ('4305',):332\n",
            " ('4369',):320\n",
            " ('4401',):259\n",
            " ('4473',):474\n",
            " ('4481',):439\n",
            " ('2439',):421\n",
            " ('4545',):285\n",
            " ('2593',):355\n",
            " ('2631',):401\n",
            " ('2671',):672\n",
            " ('4779',):360\n",
            " ('2795',):667\n",
            " ('2819',):374\n",
            " ('4869',):1304\n",
            " ('2823',):304\n",
            " ('2827',):469\n",
            " ('2829',):297\n",
            " ('2831',):352\n",
            " ('2833',):332\n",
            " ('2837',):256\n",
            " ('2839',):636\n",
            " ('2841',):660\n",
            " ('2877',):291\n",
            " ('4931',):459\n",
            " ('4935',):618\n",
            " ('3005',):320\n",
            " ('3007',):604\n",
            " ('3009',):417\n",
            " ('3015',):828\n",
            " ('3025',):408\n",
            " ('3027',):286\n",
            " ('3075',):259\n",
            " ('1039',):258\n",
            " ('1141',):351\n",
            " ('3213',):701\n",
            " ('3323',):273\n",
            " ('1279',):348\n",
            " ('3353',):313\n",
            " ('3355',):500\n",
            " ('3357',):758\n",
            " ('3397',):526\n",
            " ('3535',):292\n",
            " ('3623',):393\n",
            " ('3667',):283\n",
            " ('3669',):308\n",
            " ('3671',):266\n",
            " ('3713',):286\n",
            " ('1697',):470\n",
            " ('3769',):591\n",
            " ('3781',):433\n",
            " ('1743',):345\n",
            " ('1751',):253\n",
            " ('3837',):377\n",
            " ('3865',):412\n",
            " ('3873',):1164\n",
            " ('1829',):258\n",
            " ('1855',):456\n",
            " ('3911',):251\n",
            " ('3981',):313\n",
            " ('3983',):364\n",
            " ('4029',):270\n",
            " ('2019',):444\n",
            " ('2039',):271\n",
            " ('2995',):394\n",
            " ('4091',):307\n",
            " ('2177',):356\n",
            " ('4919',):369\n",
            " ('4425',):585\n",
            " ('4643',):302\n",
            " ('2603',):399\n",
            " ('3479',):286\n",
            " ('1615',):293\n",
            " ('2041',):365\n",
            " ('3921',):410\n",
            " ('4781',):286\n",
            " ('3483',):380\n",
            " ('4543',):323\n",
            " ('4905',):251\n",
            " ('3827',):345\n",
            " ('2037',):257\n",
            " ('2633',):294\n",
            " ('2723',):359\n",
            " ('3825',):279\n",
            " ('4581',):365\n",
            " ('1891',):295\n",
            " ('2821',):342\n",
            " ('3011',):331\n",
            "\n",
            "Frequent itemsets of length 2:\n",
            " ('1696', '3828'):258\n",
            " ('2019', '4473'):269\n",
            " ('2064', '2828'):314\n",
            " ('2568', '3970'):450\n",
            " ('2816', '2828'):332\n",
            " ('2826', '2828'):445\n",
            " ('2828', '2832'):283\n",
            " ('2828', '2838'):292\n",
            " ('2839', '2841'):311\n",
            " ('3007', '3015'):306\n",
            " ('3010', '3014'):258\n",
            " ('3355', '3357'):254\n",
            " ('3828', '4946'):253\n",
            " ('4152', '4226'):269\n",
            " ('4931', '4935'):251\n",
            " ('1104', '3357'):284\n",
            " ('1697', '3828'):254\n",
            " ('2064', '2841'):251\n",
            " ('2568', '3623'):326\n",
            " ('2568', '3873'):301\n",
            " ('2670', '2671'):303\n",
            " ('2794', '2795'):268\n",
            " ('2826', '2839'):269\n",
            " ('2826', '2841'):294\n",
            " ('2827', '2828'):276\n",
            " ('2828', '2839'):374\n",
            " ('2828', '2841'):393\n",
            " ('2828', '3015'):293\n",
            " ('2838', '2841'):272\n",
            " ('3007', '3014'):267\n",
            " ('3010', '3015'):286\n",
            " ('3014', '3015'):304\n",
            " ('3015', '3018'):273\n",
            " ('3015', '3024'):266\n",
            " ('3432', '4869'):262\n",
            " ('3623', '3970'):310\n",
            " ('3873', '3970'):305\n",
            "\n",
            "Frequent itemsets of length 3:\n",
            " ('2568', '3623', '3970'):286\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the generated rules\n",
        "print_frequent_itemsets(frequent_itemsets_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cWt_f6f9df1",
        "outputId": "f035987e-07ff-405f-ab04-50a0c9bd9df8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "******** ASSOCIATION RULES with ITEM IDs ********\n",
            "('1696',) --> ('3828',) [support: 0.0258, confidence: 0.6771653543307087]\n",
            "('2019',) --> ('4473',) [support: 0.0269, confidence: 0.6058558558558559]\n",
            "('2064',) --> ('2828',) [support: 0.0314, confidence: 0.6767241379310345]\n",
            "('3970',) --> ('2568',) [support: 0.0450, confidence: 0.734094616639478]\n",
            "('2568',) --> ('3970',) [support: 0.0450, confidence: 0.7575757575757576]\n",
            "('2816',) --> ('2828',) [support: 0.0332, confidence: 0.6372360844529751]\n",
            "('2826',) --> ('2828',) [support: 0.0445, confidence: 0.6804281345565749]\n",
            "('2832',) --> ('2828',) [support: 0.0283, confidence: 0.621978021978022]\n",
            "('2838',) --> ('2828',) [support: 0.0292, confidence: 0.6212765957446809]\n",
            "('3623',) --> ('2568',) [support: 0.0326, confidence: 0.8295165394402035]\n",
            "('2670',) --> ('2671',) [support: 0.0303, confidence: 0.6196319018404908]\n",
            "('3623',) --> ('3970',) [support: 0.0310, confidence: 0.7888040712468194]\n",
            "('3623', '3970') --> ('2568',) [support: 0.0286, confidence: 0.9225806451612903]\n",
            "('2568', '3623') --> ('3970',) [support: 0.0286, confidence: 0.8773006134969326]\n",
            "('2568', '3970') --> ('3623',) [support: 0.0286, confidence: 0.6355555555555555]\n",
            "('3623',) --> ('2568', '3970') [support: 0.0286, confidence: 0.727735368956743]\n",
            "\n",
            "******** ASSOCIATION RULES with ITEM NAMES ********\n",
            "CHARLOTTE BAG PINK POLKADOT, --> RED RETROSPOT CHARLOTTE BAG, [support: 0.0258, confidence: 0.6771653543307087]\n",
            "DOLLY GIRL LUNCH BOX, --> SPACEBOY LUNCH BOX, [support: 0.0269, confidence: 0.6058558558558559]\n",
            "DOTCOM POSTAGE, --> JUMBO BAG RED RETROSPOT, [support: 0.0314, confidence: 0.6767241379310345]\n",
            "ROSES REGENCY TEACUP AND SAUCER, --> GREEN REGENCY TEACUP AND SAUCER, [support: 0.0450, confidence: 0.734094616639478]\n",
            "GREEN REGENCY TEACUP AND SAUCER, --> ROSES REGENCY TEACUP AND SAUCER, [support: 0.0450, confidence: 0.7575757575757576]\n",
            "JUMBO  BAG BAROQUE BLACK WHITE, --> JUMBO BAG RED RETROSPOT, [support: 0.0332, confidence: 0.6372360844529751]\n",
            "JUMBO BAG PINK POLKADOT, --> JUMBO BAG RED RETROSPOT, [support: 0.0445, confidence: 0.6804281345565749]\n",
            "JUMBO BAG STRAWBERRY, --> JUMBO BAG RED RETROSPOT, [support: 0.0283, confidence: 0.621978021978022]\n",
            "JUMBO BAG WOODLAND ANIMALS, --> JUMBO BAG RED RETROSPOT, [support: 0.0292, confidence: 0.6212765957446809]\n",
            "PINK REGENCY TEACUP AND SAUCER, --> GREEN REGENCY TEACUP AND SAUCER, [support: 0.0326, confidence: 0.8295165394402035]\n",
            "HEART OF WICKER LARGE, --> HEART OF WICKER SMALL, [support: 0.0303, confidence: 0.6196319018404908]\n",
            "PINK REGENCY TEACUP AND SAUCER, --> ROSES REGENCY TEACUP AND SAUCER, [support: 0.0310, confidence: 0.7888040712468194]\n",
            "PINK REGENCY TEACUP AND SAUCER, ROSES REGENCY TEACUP AND SAUCER, --> GREEN REGENCY TEACUP AND SAUCER, [support: 0.0286, confidence: 0.9225806451612903]\n",
            "GREEN REGENCY TEACUP AND SAUCER, PINK REGENCY TEACUP AND SAUCER, --> ROSES REGENCY TEACUP AND SAUCER, [support: 0.0286, confidence: 0.8773006134969326]\n",
            "GREEN REGENCY TEACUP AND SAUCER, ROSES REGENCY TEACUP AND SAUCER, --> PINK REGENCY TEACUP AND SAUCER, [support: 0.0286, confidence: 0.6355555555555555]\n",
            "PINK REGENCY TEACUP AND SAUCER, --> GREEN REGENCY TEACUP AND SAUCER, ROSES REGENCY TEACUP AND SAUCER, [support: 0.0286, confidence: 0.727735368956743]\n"
          ]
        }
      ],
      "source": [
        "# Generate association rules\n",
        "association_rules = generate_association_rules(frequent_itemsets_all, min_confidence)\n",
        "\n",
        "# Print the generated association rules\n",
        "print_association_rules(association_rules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdOw7FCy8-8G",
        "outputId": "cffaa879-22ba-4037-85b8-9261bbc5c28c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** Rules fired ***\n",
            "('2826',) --> ('2828',)\n",
            "('3623',) --> ('2568',)\n",
            "('3623',) --> ('3970',)\n",
            "('3623',) --> ('2568', '3970')\n",
            "\n",
            "Recommended item IDs:\n",
            "2568\n",
            "3970\n",
            "2828\n",
            "\n",
            "Recommended item names:\n",
            "GREEN REGENCY TEACUP AND SAUCER\n",
            "ROSES REGENCY TEACUP AND SAUCER\n",
            "JUMBO BAG RED RETROSPOT\n"
          ]
        }
      ],
      "source": [
        "# Given a new transaction\n",
        "new_transaction = [2826, 2839, 1002, 3623]\n",
        "\n",
        "# Make recommendations\n",
        "recommended_items = recommend(new_transaction, association_rules)\n",
        "\n",
        "# Print the item IDs and names of the recommended items\n",
        "print_recommended_items(recommended_items)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
